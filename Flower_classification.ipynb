{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaidosa/Flowers-Species/blob/main/Flower_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flower Classification\n",
        "*To accomplish this task we've taken Flower dataset from Kaggel: https://www.kaggle.com/alxmamaev/flowers-recognition*"
      ],
      "metadata": {
        "id": "4AIDxfGVqOIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5Tkz6Fwql4R",
        "outputId": "caeb6474-8307-4224-8671-1c8c2bba2c9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/data/kaggle\""
      ],
      "metadata": {
        "id": "LOJ9UcmOsn0p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Moving to the directory where the kaggle.json file is present (You need API to download dataset directly\n",
        "from kaggle it's an easy process) \"\"\"\n",
        "%cd /content/gdrive/MyDrive/data/kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL0pD232rw10",
        "outputId": "f31af27b-2e9e-4d76-9f65-582fe327a325"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/data/kaggle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d alxmamaev/flowers-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4nADxv3rVHF",
        "outputId": "8320389e-eee2-4141-837b-a70f8da78564"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flowers-recognition.zip to /content/gdrive/MyDrive/data/kaggle\n",
            "100% 225M/225M [00:11<00:00, 24.3MB/s]\n",
            "100% 225M/225M [00:11<00:00, 21.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the data files and deleting the zip file\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "id": "qiLwxyenuYhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "VQkVziZPuxue"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "186SYzHLQsVQ"
      },
      "outputs": [],
      "source": [
        "mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "std  = np.array([0.2023, 0.1994, 0.2010])\n",
        "\n",
        "data_transforms = {\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.CenterCrop((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'test':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.CenterCrop((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YUFf3VSTQpgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d17842-4cf0-4c63-d415-f509bff4782b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "batch_size = 8\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 50\n",
        "num_classes = 5\n",
        "\n",
        "# If GPU is available choose that, below code is for the same\n",
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "znFTMXl7xg2W",
        "outputId": "683d8ef0-c106-4a9a-c35e-af4be94084da"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/data/kaggle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "b73XdehQRRsd"
      },
      "outputs": [],
      "source": [
        "Path_data = '/content/gdrive/MyDrive/data/kaggle/flowers/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX517_0Le0zK",
        "outputId": "b0dfd61b-9b5f-4151-fea3-64f64369bf50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4317\n",
            "torch.Size([3, 224, 224])\n",
            "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}\n"
          ]
        }
      ],
      "source": [
        "# Total datset\n",
        "data_set = torchvision.datasets.ImageFolder(Path_data, transform=data_transforms['train'])\n",
        "\n",
        "#size of the total dataset\n",
        "print(len(data_set))\n",
        "# shape of the dataset\n",
        "print(data_set[0][0].shape)\n",
        "# What are the classes of the dataset\n",
        "print(data_set.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt the data between train and the validation sets\n",
        "\n",
        "Split_ratio = 0.8\n",
        "train_size = int(Split_ratio * len(data_set))\n",
        "val_size   = len(data_set) - train_size\n",
        "\n",
        "print(f'Train size is: {train_size}, Validation size is {val_size}')\n",
        "\n",
        "train_data, val_data = torch.utils.data.random_split(data_set, [train_size, val_size])\n",
        "print(f\"{len(train_data)}, {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_4cgo6Eyvqi",
        "outputId": "49a245b5-9f0b-41bd-9d66-99f2f7d10298"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size is: 3453, Validation size is 864\n",
            "3453, 864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders\n",
        "train_load = DataLoader(dataset=train_data,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "\n",
        "val_load = DataLoader(dataset=val_data,\n",
        "                      batch_size=1,\n",
        "                      shuffle=True,\n",
        "                      num_workers=2)"
      ],
      "metadata": {
        "id": "FWRc8-m40Iz6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the data loading is working\n",
        "\n",
        "eg = iter(train_load)\n",
        "sample, labels = next(eg)\n",
        "print(sample.shape, labels.shape)\n",
        "print(f\"{len(train_load)}, {len(val_load)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cKWSZOY0336",
        "outputId": "e325c8a9-2975-4cb2-948d-f36d2a65df4e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3, 224, 224]) torch.Size([8])\n",
            "432, 864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CNN models class\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self, model, num_classes):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.base_model = nn.Sequential(*list(model.children())[:-1]) #excluding the last FC layer\n",
        "    self.linear1 = nn.Linear(in_features=2048, out_features=512)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(in_features=512, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.base_model(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    lin = self.linear1(x)\n",
        "    x = self.relu(lin)\n",
        "    out = self.linear2(x)\n",
        "    return lin, out"
      ],
      "metadata": {
        "id": "yoEFkrbp17U-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet50(pretrained=True) # This our base model\n",
        "\n",
        "model = ConvNet(model, num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "objective_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3faOO1S3K7w",
        "outputId": "000f8f1b-3388-47fa-c66e-f553dcaf91fd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWEftYfP31ms",
        "outputId": "0444c0ea-5a1c-4a99-c3ba-6913542e7133"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConvNet(\n",
            "  (base_model): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (7): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (linear1): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (linear2): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training on our flower dataset\n",
        "\n",
        "n_iters = len(train_load)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for ii,(images, labels) in enumerate(train_load):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    _,outputs = model(images)\n",
        "    loss = objective_function(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (ii+1)%108 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Step[{ii+1}/{n_iters}], Loss = {loss.item():.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0DuIC643-u7",
        "outputId": "cf0c7213-8557-4600-e6e5-115e80f9b361"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Step[108/432], Loss = 0.664136\n",
            "Epoch [1/50], Step[216/432], Loss = 0.222699\n",
            "Epoch [1/50], Step[324/432], Loss = 0.354514\n",
            "Epoch [1/50], Step[432/432], Loss = 0.736255\n",
            "Epoch [2/50], Step[108/432], Loss = 0.928168\n",
            "Epoch [2/50], Step[216/432], Loss = 0.021836\n",
            "Epoch [2/50], Step[324/432], Loss = 0.112790\n",
            "Epoch [2/50], Step[432/432], Loss = 0.110380\n",
            "Epoch [3/50], Step[108/432], Loss = 0.834857\n",
            "Epoch [3/50], Step[216/432], Loss = 0.062991\n",
            "Epoch [3/50], Step[324/432], Loss = 0.101490\n",
            "Epoch [3/50], Step[432/432], Loss = 0.150484\n",
            "Epoch [4/50], Step[108/432], Loss = 0.290846\n",
            "Epoch [4/50], Step[216/432], Loss = 0.159907\n",
            "Epoch [4/50], Step[324/432], Loss = 0.025264\n",
            "Epoch [4/50], Step[432/432], Loss = 0.031802\n",
            "Epoch [5/50], Step[108/432], Loss = 0.567587\n",
            "Epoch [5/50], Step[216/432], Loss = 0.100825\n",
            "Epoch [5/50], Step[324/432], Loss = 0.307965\n",
            "Epoch [5/50], Step[432/432], Loss = 0.326450\n",
            "Epoch [6/50], Step[108/432], Loss = 0.010951\n",
            "Epoch [6/50], Step[216/432], Loss = 0.303842\n",
            "Epoch [6/50], Step[324/432], Loss = 0.040656\n",
            "Epoch [6/50], Step[432/432], Loss = 0.006368\n",
            "Epoch [7/50], Step[108/432], Loss = 0.006204\n",
            "Epoch [7/50], Step[216/432], Loss = 0.225704\n",
            "Epoch [7/50], Step[324/432], Loss = 0.028404\n",
            "Epoch [7/50], Step[432/432], Loss = 0.506706\n",
            "Epoch [8/50], Step[108/432], Loss = 0.064392\n",
            "Epoch [8/50], Step[216/432], Loss = 0.065346\n",
            "Epoch [8/50], Step[324/432], Loss = 0.152601\n",
            "Epoch [8/50], Step[432/432], Loss = 0.133470\n",
            "Epoch [9/50], Step[108/432], Loss = 0.000515\n",
            "Epoch [9/50], Step[216/432], Loss = 0.086606\n",
            "Epoch [9/50], Step[324/432], Loss = 0.004285\n",
            "Epoch [9/50], Step[432/432], Loss = 0.242830\n",
            "Epoch [10/50], Step[108/432], Loss = 0.025454\n",
            "Epoch [10/50], Step[216/432], Loss = 0.306864\n",
            "Epoch [10/50], Step[324/432], Loss = 0.001727\n",
            "Epoch [10/50], Step[432/432], Loss = 0.000380\n",
            "Epoch [11/50], Step[108/432], Loss = 0.026466\n",
            "Epoch [11/50], Step[216/432], Loss = 0.023672\n",
            "Epoch [11/50], Step[324/432], Loss = 0.024090\n",
            "Epoch [11/50], Step[432/432], Loss = 0.001935\n",
            "Epoch [12/50], Step[108/432], Loss = 0.004956\n",
            "Epoch [12/50], Step[216/432], Loss = 0.249853\n",
            "Epoch [12/50], Step[324/432], Loss = 0.003247\n",
            "Epoch [12/50], Step[432/432], Loss = 0.005500\n",
            "Epoch [13/50], Step[108/432], Loss = 0.107201\n",
            "Epoch [13/50], Step[216/432], Loss = 0.000631\n",
            "Epoch [13/50], Step[324/432], Loss = 0.000388\n",
            "Epoch [13/50], Step[432/432], Loss = 0.000265\n",
            "Epoch [14/50], Step[108/432], Loss = 0.001033\n",
            "Epoch [14/50], Step[216/432], Loss = 0.000656\n",
            "Epoch [14/50], Step[324/432], Loss = 0.004364\n",
            "Epoch [14/50], Step[432/432], Loss = 0.007413\n",
            "Epoch [15/50], Step[108/432], Loss = 0.009931\n",
            "Epoch [15/50], Step[216/432], Loss = 0.002391\n",
            "Epoch [15/50], Step[324/432], Loss = 0.000150\n",
            "Epoch [15/50], Step[432/432], Loss = 0.042672\n",
            "Epoch [16/50], Step[108/432], Loss = 0.014459\n",
            "Epoch [16/50], Step[216/432], Loss = 0.000945\n",
            "Epoch [16/50], Step[324/432], Loss = 0.000715\n",
            "Epoch [16/50], Step[432/432], Loss = 0.004713\n",
            "Epoch [17/50], Step[108/432], Loss = 0.009644\n",
            "Epoch [17/50], Step[216/432], Loss = 0.008201\n",
            "Epoch [17/50], Step[324/432], Loss = 0.000103\n",
            "Epoch [17/50], Step[432/432], Loss = 0.000062\n",
            "Epoch [18/50], Step[108/432], Loss = 0.007828\n",
            "Epoch [18/50], Step[216/432], Loss = 0.029756\n",
            "Epoch [18/50], Step[324/432], Loss = 0.002462\n",
            "Epoch [18/50], Step[432/432], Loss = 0.011252\n",
            "Epoch [19/50], Step[108/432], Loss = 0.002619\n",
            "Epoch [19/50], Step[216/432], Loss = 0.077371\n",
            "Epoch [19/50], Step[324/432], Loss = 0.002330\n",
            "Epoch [19/50], Step[432/432], Loss = 0.037248\n",
            "Epoch [20/50], Step[108/432], Loss = 0.000245\n",
            "Epoch [20/50], Step[216/432], Loss = 0.004155\n",
            "Epoch [20/50], Step[324/432], Loss = 0.005375\n",
            "Epoch [20/50], Step[432/432], Loss = 0.000471\n",
            "Epoch [21/50], Step[108/432], Loss = 0.098885\n",
            "Epoch [21/50], Step[216/432], Loss = 0.000034\n",
            "Epoch [21/50], Step[324/432], Loss = 0.003262\n",
            "Epoch [21/50], Step[432/432], Loss = 0.154642\n",
            "Epoch [22/50], Step[108/432], Loss = 0.000319\n",
            "Epoch [22/50], Step[216/432], Loss = 0.002021\n",
            "Epoch [22/50], Step[324/432], Loss = 0.018361\n",
            "Epoch [22/50], Step[432/432], Loss = 0.125734\n",
            "Epoch [23/50], Step[108/432], Loss = 0.000898\n",
            "Epoch [23/50], Step[216/432], Loss = 0.000510\n",
            "Epoch [23/50], Step[324/432], Loss = 0.000532\n",
            "Epoch [23/50], Step[432/432], Loss = 0.000289\n",
            "Epoch [24/50], Step[108/432], Loss = 0.108163\n",
            "Epoch [24/50], Step[216/432], Loss = 0.000362\n",
            "Epoch [24/50], Step[324/432], Loss = 0.000223\n",
            "Epoch [24/50], Step[432/432], Loss = 0.000034\n",
            "Epoch [25/50], Step[108/432], Loss = 0.000167\n",
            "Epoch [25/50], Step[216/432], Loss = 0.000098\n",
            "Epoch [25/50], Step[324/432], Loss = 0.013946\n",
            "Epoch [25/50], Step[432/432], Loss = 0.008068\n",
            "Epoch [26/50], Step[108/432], Loss = 0.000359\n",
            "Epoch [26/50], Step[216/432], Loss = 0.000148\n",
            "Epoch [26/50], Step[324/432], Loss = 0.000087\n",
            "Epoch [26/50], Step[432/432], Loss = 0.000998\n",
            "Epoch [27/50], Step[108/432], Loss = 0.000346\n",
            "Epoch [27/50], Step[216/432], Loss = 0.000036\n",
            "Epoch [27/50], Step[324/432], Loss = 0.001969\n",
            "Epoch [27/50], Step[432/432], Loss = 0.002069\n",
            "Epoch [28/50], Step[108/432], Loss = 0.041516\n",
            "Epoch [28/50], Step[216/432], Loss = 0.000567\n",
            "Epoch [28/50], Step[324/432], Loss = 0.055370\n",
            "Epoch [28/50], Step[432/432], Loss = 0.000245\n",
            "Epoch [29/50], Step[108/432], Loss = 0.000214\n",
            "Epoch [29/50], Step[216/432], Loss = 0.000098\n",
            "Epoch [29/50], Step[324/432], Loss = 0.000459\n",
            "Epoch [29/50], Step[432/432], Loss = 0.001033\n",
            "Epoch [30/50], Step[108/432], Loss = 0.007928\n",
            "Epoch [30/50], Step[216/432], Loss = 0.011851\n",
            "Epoch [30/50], Step[324/432], Loss = 0.002371\n",
            "Epoch [30/50], Step[432/432], Loss = 0.000901\n",
            "Epoch [31/50], Step[108/432], Loss = 0.001389\n",
            "Epoch [31/50], Step[216/432], Loss = 0.000189\n",
            "Epoch [31/50], Step[324/432], Loss = 0.000224\n",
            "Epoch [31/50], Step[432/432], Loss = 0.172090\n",
            "Epoch [32/50], Step[108/432], Loss = 0.000303\n",
            "Epoch [32/50], Step[216/432], Loss = 0.000275\n",
            "Epoch [32/50], Step[324/432], Loss = 0.002255\n",
            "Epoch [32/50], Step[432/432], Loss = 0.028910\n",
            "Epoch [33/50], Step[108/432], Loss = 0.010765\n",
            "Epoch [33/50], Step[216/432], Loss = 0.000444\n",
            "Epoch [33/50], Step[324/432], Loss = 0.000036\n",
            "Epoch [33/50], Step[432/432], Loss = 0.003937\n",
            "Epoch [34/50], Step[108/432], Loss = 0.000042\n",
            "Epoch [34/50], Step[216/432], Loss = 0.002780\n",
            "Epoch [34/50], Step[324/432], Loss = 0.003347\n",
            "Epoch [34/50], Step[432/432], Loss = 0.000189\n",
            "Epoch [35/50], Step[108/432], Loss = 0.000138\n",
            "Epoch [35/50], Step[216/432], Loss = 0.001273\n",
            "Epoch [35/50], Step[324/432], Loss = 0.000106\n",
            "Epoch [35/50], Step[432/432], Loss = 0.001422\n",
            "Epoch [36/50], Step[108/432], Loss = 0.001835\n",
            "Epoch [36/50], Step[216/432], Loss = 0.002288\n",
            "Epoch [36/50], Step[324/432], Loss = 0.000157\n",
            "Epoch [36/50], Step[432/432], Loss = 0.024959\n",
            "Epoch [37/50], Step[108/432], Loss = 0.000148\n",
            "Epoch [37/50], Step[216/432], Loss = 0.000003\n",
            "Epoch [37/50], Step[324/432], Loss = 0.000080\n",
            "Epoch [37/50], Step[432/432], Loss = 0.000434\n",
            "Epoch [38/50], Step[108/432], Loss = 0.002099\n",
            "Epoch [38/50], Step[216/432], Loss = 0.000091\n",
            "Epoch [38/50], Step[324/432], Loss = 0.105758\n",
            "Epoch [38/50], Step[432/432], Loss = 0.000195\n",
            "Epoch [39/50], Step[108/432], Loss = 0.000223\n",
            "Epoch [39/50], Step[216/432], Loss = 0.000172\n",
            "Epoch [39/50], Step[324/432], Loss = 0.000002\n",
            "Epoch [39/50], Step[432/432], Loss = 0.000035\n",
            "Epoch [40/50], Step[108/432], Loss = 0.078509\n",
            "Epoch [40/50], Step[216/432], Loss = 0.000102\n",
            "Epoch [40/50], Step[324/432], Loss = 0.000118\n",
            "Epoch [40/50], Step[432/432], Loss = 0.000030\n",
            "Epoch [41/50], Step[108/432], Loss = 0.000010\n",
            "Epoch [41/50], Step[216/432], Loss = 0.000050\n",
            "Epoch [41/50], Step[324/432], Loss = 0.000010\n",
            "Epoch [41/50], Step[432/432], Loss = 0.000879\n",
            "Epoch [42/50], Step[108/432], Loss = 0.000016\n",
            "Epoch [42/50], Step[216/432], Loss = 0.001824\n",
            "Epoch [42/50], Step[324/432], Loss = 0.000535\n",
            "Epoch [42/50], Step[432/432], Loss = 0.000029\n",
            "Epoch [43/50], Step[108/432], Loss = 0.000002\n",
            "Epoch [43/50], Step[216/432], Loss = 0.000453\n",
            "Epoch [43/50], Step[324/432], Loss = 0.000235\n",
            "Epoch [43/50], Step[432/432], Loss = 0.000111\n",
            "Epoch [44/50], Step[108/432], Loss = 0.000510\n",
            "Epoch [44/50], Step[216/432], Loss = 0.138621\n",
            "Epoch [44/50], Step[324/432], Loss = 0.014148\n",
            "Epoch [44/50], Step[432/432], Loss = 0.001028\n",
            "Epoch [45/50], Step[108/432], Loss = 0.000391\n",
            "Epoch [45/50], Step[216/432], Loss = 0.000111\n",
            "Epoch [45/50], Step[324/432], Loss = 0.002150\n",
            "Epoch [45/50], Step[432/432], Loss = 0.029741\n",
            "Epoch [46/50], Step[108/432], Loss = 0.019239\n",
            "Epoch [46/50], Step[216/432], Loss = 0.000066\n",
            "Epoch [46/50], Step[324/432], Loss = 0.001070\n",
            "Epoch [46/50], Step[432/432], Loss = 0.000778\n",
            "Epoch [47/50], Step[108/432], Loss = 0.003582\n",
            "Epoch [47/50], Step[216/432], Loss = 0.000333\n",
            "Epoch [47/50], Step[324/432], Loss = 0.001357\n",
            "Epoch [47/50], Step[432/432], Loss = 0.000906\n",
            "Epoch [48/50], Step[108/432], Loss = 0.002180\n",
            "Epoch [48/50], Step[216/432], Loss = 0.000073\n",
            "Epoch [48/50], Step[324/432], Loss = 0.000059\n",
            "Epoch [48/50], Step[432/432], Loss = 0.000007\n",
            "Epoch [49/50], Step[108/432], Loss = 0.000423\n",
            "Epoch [49/50], Step[216/432], Loss = 0.000019\n",
            "Epoch [49/50], Step[324/432], Loss = 0.000061\n",
            "Epoch [49/50], Step[432/432], Loss = 0.000332\n",
            "Epoch [50/50], Step[108/432], Loss = 0.000046\n",
            "Epoch [50/50], Step[216/432], Loss = 0.000057\n",
            "Epoch [50/50], Step[324/432], Loss = 0.000195\n",
            "Epoch [50/50], Step[432/432], Loss = 0.000050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating model\n",
        "\n",
        "def eval_model(model, dataloader, phase):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # for the entire dataset\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      _, outputs = model(images)\n",
        "\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = n_correct/float(n_samples)\n",
        "\n",
        "    print(f'Accuracy of model on {phase} set = {(100.0 * accuracy):.4f} %')\n",
        ""
      ],
      "metadata": {
        "id": "6JyQJutv7zyY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_load = DataLoader(dataset=train_data,\n",
        "                        batch_size=1,\n",
        "                        shuffle=False,\n",
        "                        num_workers=2)\n",
        "\n",
        "eval_model(model, dataloader=train_load, phase='training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIScMkeN-ND8",
        "outputId": "fbed2e39-76ea-4b97-d3e4-cabc6a06c11b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model on training set = 99.9421 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model, dataloader=val_load, phase='testing')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4ULyhTA-j63",
        "outputId": "dde56dc5-c09e-4cfe-b3c6-de1a50aeca6a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model on testing set = 93.5185 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mSVw8meRYWr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "18jvVGeVkYe2sDXtJoDUVVBGp836kSMXi",
      "authorship_tag": "ABX9TyP9rw85b19wUBMcDTERU8/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}